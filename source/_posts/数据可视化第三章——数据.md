title: 数据可视化第三章——数据
tags: []
categories:
  - 数据可视化
date: 2019-08-24 09:28:00
author:
---
#### 3.1 数据释义
数据是符合的集合，是表达表达客观事物未加工的原始素材，例如，图形、符号、数字、字母等都是数据的不同形式。数据模型是用来描述数据表达的底层描述模型，它包含数据的定义和类型，以及不同类型数据的操作功能，例如，浮点数类型可以配备加、减、乘、除操作等。与数据模型对应的是概念模型，它对目标事物的状态和行为进行抽象的语义描述，并提供构建、推理支持等操作。例如，一维浮点数可以描述温度概念，三维浮点数向量可以描述空间的风向概念。 
<!--more--> 
数据也可以看成是数据对象和其属性的集合，其中属性可被看成是变量、值域、特征或特性，例如，人类头发的颜色、人类体温等。单个数据对象可以由一组属性描述，也被称为记录、点、实例、采样、实体等。属性值可以是表达属性的任意数值或符号，同一类属性可以具有不同的属性值，例如，长度的度量单位可以是英尺或米。不同的属性也可能具有相同的取值和不同的含义，例如，年份和年龄都是整数型数值，而年龄通常有取值区间。

#### 3.1.1 数据基础

###### 3.1.1.1 数据分类
数据的分类和信息与知识的分类相关。从关系模型的角度讲，数据可被分为实体和关系两部分。实体是被可视化的对象：关系定义了实体与其他实体之间关系的结构和模式。关系可被显式地定义，也可在可视化过程中逐步挖掘。实体或关系可以配备属性，例如，一个苹果的颜色可以看做它的属性。实体、关系和属性在数据库设计中被广泛使用，形成了关系型数据库的基础。  
实体关系模型能描述数据之间的结构，但不考虑基于实体、关系和属性的操作。常规的数据操作包括：数值计算；数据列表的插入、融合与删除；取反；生成新的实体或关系；实体的变换；从其他对象中形成新对象；单个实体拆分成组件。  
数据属性可分为离散属性和连续属性。离散属性的取值来自于有限或可数的集合，例如邮政编码、等级、文档单词等；连续属性则对应于实数域，例如温度、高度和湿度等。在测量和计算机表示时，实数表示的精度受限于所采用的数值精度（例如，双精度浮方法点数采用 64 位）。  
针对这些基本数据类型在交互方法主要有：概括、缩放、过滤、查看细节、关联、查看历史和提取等，详见第 13 章。这些基本任务构成了可视语言设计的基础。

###### 3.1.1.2 数据集
数据集是数据的实例。常见的数据集的表达形式有三类。
+ 数据记录集  
数据记录由一组包含固定属性值的数据元素组成。数据记录主要有三种形式：数据矩阵、文档向量表示和事物处理数据。  
如果数据对象具有一组固定的数值属性，则数据对象可视为高维空间的点集，每个维度对应单个属性。这种数据集可以直接表达为一个 m×n 的矩阵。其中，矩阵的每行代表一个对象，每列代表单个属性在数据集中的分布。这种表示方法称为数据矩阵。数据矩阵通常采用表单（spreadsheet）进行组织，又称为电子表单。常见的电子表单可以采用枢轴表（Pivot Tables）技术进行自动排序、计数和总计，生成另外的表单。从本质上讲，这是一种简化的联机分析处理（OLAP）技术。  
文档是单词的集合。如果统计文档中所有单词出现的频率，则一个文档可以被表示为一个向量，其长度是单词集的个数，每个分量记录单词集中每个单词在该文档中的频率。  
事务处理数据是一类特殊的数据记录，每个记录包含一组数据项。例如，一组超市购物的事务处理数据是：（西瓜，梨子，苹果）、（洗发水，苹果，核桃，香蕉）、（香烟，西瓜，口香糖，笔记本，脸盆）。事务处理数据与数据矩阵的差别在于，事务处理数据的每个记录包含的个数和属性不固定，因此无法用矩阵这种大小确定的方式进行表达。
+ 图数据集  
图是一种非结构化的数据结构，由一组节点和一组连接两个节点之间的加权边组成。常见的图数据有表达城市之间航空路线的世界航线图、万维网链接图、化学分子式等。树是一种没有回路的连通图，是任意两个顶点间有且只有一条路径的图，第 9 章将详细阐述图数据的可视化。
+ 有序数据集  
有序数据是具有某种顺序的数据集。常见的数据集包括空间数据、时间数据、时空数据、顺序数据和基因测序数据等。  
在某些场合（如科学可视化），数据可以根据数据的维度进行分类：标量（一维点）、向量（多维点）、张量（矩阵）等。  
数据集的另一种分类是考查数据模型的结构。可以用二维表结构逻辑表达实现的数据称为结构化数据，如数据矩阵（二维表单）；反之，难以采用数据库二维逻辑表表达的数据称为非结构化数据，如图数据、文本、图像、音频和视频等。半结构化数据是介于结构化数据（如关系型数据库、面向对象数据库中的数据）和完全无结构数据（如声音、图像文件等）之间的数据，如 XML 文档。随着互联网的飞速发展，大量的文本和多媒体内容被不断地产生和接收，非结构化数据日趋广泛。

###### 3.1.1.3 数据相似度与密度
相似度（similarity）是衡量多个数据对象之间相似的数值，通常位于 0 和 12 之间。与之对应的测度是相异度（dissimilarity），其下限是 0，上限与数据集有关，可能超过 1。临近度是相似度和相异度的统一描述。  
计算相似度有很多种方法，一些常用的距离和相似度定义有：
+ 欧几里德距离。
+ 明科夫斯基距离（欧几里德距离的推广）。
+ 余弦距离
+ Jaccard 相似度  
如果数据对象的属性具有多种类型，则可为每个属性计算相似度，再进行加权平均。  
在基于密度的数据聚类时,需要衡量数据为的密度，通常定义有三类：
+ 欧几里德密度(单位区域内的点的数目)。
+ 概率密度。
+ 基于图结构的密度。 
在第一类方法中，最简单的方法是将区域分成等分，统计每个部分包含的点的数目。另一种基于中心的欧几里德密度定义为该点固定尺寸邻域中的点的数目。
###### 3.1.2 数据科学及过程
我们身处数据为王的时代。在我们身处的世界中，信息量与日俱增，每天都有大量的数据在我们身边被创建、复制和传输。IDC 于 2011 年发布的一项统计表明，在过去 5 年中，每年全世界数据总量均比前一年翻一番，并且 2011 年全年世界创建和复制的数据总量已达到 1.8ZB（1ZB=109TB）。通常来说，拥有更多的数据意味更多的价值，然而当前的信息处理和分析手段却远落后于数据获取的速度。  
在这样的背景下，麦肯锡公司在 2011 年开始提出“大数据时代”。“大数据”通常指无法在现有能力和工具的支持下，在可接受的时间范围内进行采集、管理和处理的数据，其特征如下。
+ 海量的数据规模。海量的数据源源不断地产生、存储和消费。随着数据采集方式和存储设备的不断更新，我们所保存的网页数据、电子商务数据、金融交易数据等数据开始快速积累起来。
+ 快速的数据流转、动态的数据体系。数据量的增大和数据产生速度的加快决定了在大数据时代我们需要面对快速的数据流转。各种各样的传感器、监控摄像头等数据采集设备给人们带来巨大的采集数据流，每天在因特网上产生和消失的网站及数据也构成了高速变更中的数据体系。对于当前人们无法承受的数据流动和变更速度来说，如何存储、管理、分析这些数据成了一个棘手的问题。
+ 多样的数据。当前我们能够遇到的大数据通常是没有统一定义的、非结构化数据，这意味着这些数据的存储格式、组织形式以及数据间的关系没有一个统一的数据模型来描述。如何有效地应对以结构化、非结构化数据组成的异构数据体系，是大数据时代处理复杂数据的重要议题之一。
+ 巨大的数据价值。数据获取和数据计算设备越来越强大和廉价，这使得以计算的手段从数据中挖掘出应用价值成为可能和必然。例如，网站可以利用用户行为数据为用户提供个性化服务，公司可以基于商业数据开发数据产品作为用户增值服务等。  
互联网时代，许多拥有大量客户数据的 IT 企业和金融企业开始意识到，他们所积累务的商业数据能够发挥的作用与日俱增。商业数据分析人员可以基于客户数据发现潜在的市场，或者对顾客行为习惯进行分析以提供更具有针对性的服务。在大数据时代，这些海量数据已经开始成为 IT 企业的核心竞争力之一。截至 2012 年 6 月，Facebook 用户数量已达到 9.55亿，每天用户在网站上的活动都会导致海量行为数据的产生，这为人类行为分析提供了契机。Facebook 内部的“数据科学小组”已经开展了很多研究活动，以期从海量人类社会行为数据中寻找到可利用的模式，推动人类对自身行为的认识进程。对于一些 B2C 网站，大数据的研究和基于数据的产品服务也是企业的重要发展方向之一（图 3.1）。金融服务类机构同样拥有大量客户数据，通过对大数据进行分析、挖掘，银行能够有效地发现金融欺诈行为，而保险公司可以从客户数据中发现潜在的“优质客户”，并分析出适合这些客户的保险产品,进行个性化营销。  
在科学研究领域，传统的科学探究模式正在遭受来自大数据的强烈冲击。随着技术的不断推进，诸如卫星上的远程传感器、天空望远镜、生物显微镜以及大规模科学计等设备和实验都会实时产生出海量数据流，如图 3.2 所示。海量科学数据的产生将科学研究推进到一个新的模式，数据在科学探索中开始发挥越来越大的作用。2012 年 3 月，美国国家卫生研究院宣布世界最大的遗传变异研究数据集由 Amazon 网站提供云服务支持，其 200TB 的人类遗传变异数据将存储于 Amazon 云端供研究人员进行免费查询和分析。然而，现在科学研究人员遇到了新的挑战，即在拥有大型数据集的同时，他们也需要对应这种数据密度的软件工具和高性能计算资源，以协助进行基干数据的科学研究。  
数据在政府管理、国家安全等领域的价值也越来越明显。从 2009 年起，美国政府通过 http://www.data.gov 数据网站开始向公众提供各类政府数据。几乎同时，联合国推出了“全球脉动”项目（ http://www.unglobalpulse.org/ ），期望利用大数据促进全球经济发展。例如，采用基于数据的情绪分析方法分析社交网站内容，可以预测某些重要事件的经济发展趋势。同时，国家战略政策方针的制定也开始依赖大数据和数据科学，期望从数据中能够寻找到支持国家决策的有效信息。2012 年 3 月 29 日，美国白宫科学和技术政策办公室发布了《大数据研究与发展倡议》（Big Data Research and Development Initiative），通过实施政府数据集开放化，推动数据和统计课程在教育计划中的比重，以及建立针对某些特定
方面数据的持续收集方法等措施，来促进大数据为国家所更好地使用。  
在服务科学蓬勃发展的今天，我们也开始走向“数据即服务（DaaS）”的时代。用户可以随时随地按需求获取数据和信息。海量数据带来了相应的海量数据处理及分析需求。然而，传统方法难以应对海量原始数据的直接处理和分析，在很多情况下数据被淹没于浩瀚的“数据海洋”中，这些被淹没的数据中不乏能够提供有价值信息的数据，因此我们在解决大数据获取、存储等问题的同时，亟需一种能够针对大数据进行统计、分析和信息提取的方法。近年来，以数据为原材料的电子科学、信息科学、语义网络、数据组织与管理、数据分析、数据挖掘和数据可视化等手段，可以有效地提取隐藏在数据中有价值的信息，并且将数据利用质量提高到传统方法所不能及的高度，是提炼科学原理、验证科学假设、服务科学探索的新型思路。现在，研究这种综合性方法的学科被称为“数据科学”，是 21 世纪新兴的一门交叉学科。数据科学涵盖了数据管理、图书馆科学、计算机科学、统计学、视觉设计、可视化、人机交互，以及基于架构式和信息技术的物理科学。它改变了所有学科个人和协作工作的模式，使得无论是商业还是科学数据分析处理都上升到一人新的“数据驱动”的阶段，帮助数据分析师和科学家解决尺度、复杂度超越已有的所有工具承受范围的全局问题。  
Tony Hey、Stewart Tansley 和 Kristin Tolle 主编的著作《第四范式：数据密集型的科学发现》定义了数据科学的一些基本主题。  
+ 数据和信息的历史
+ 数据、信息、知识概念和最新进展
+ 数据与信息科学的学术基础
+ 信息学简介
+ 科学的数据生命周期
+ 数据获取、保存和保护
+ 数据集成
+ 元数据
+ 数据模型和架构
+ 数据工具、基于数据的服务范式
+ 数据网、网页上的数据、深层网
+ 数据工作流管理
+ 数据可视化
+ 数据发现
+ 数据和信息管理  
同时，该著作从应用角度出发，提出了当今适合使用数据科学的研究领域，包括地球科学、生物、天文、环境与气候、化学、物理、航空、环境工程、数据图书馆和科学出版、商业、社会学、经济等。  
作为数据内涵信息的展示方法和人机交互接口，数据可视化已成为数据科学的素之一。面对海量数据，大多数时候我们很难通过直接观察数据本身，或者对数据进行简单统计分析后得到数据中蕴涵的信息。例如，我们无法通过查看海量的服务器日志来判断系统是否遭到攻击威胁，或者简单统计交友网站上所有的好友关系来发掘用户的喜好等。海量的数据通过可视化方法变成形象、生动的图形，有助于人类对数据中的属性、关系进行深入探究，利用人类智慧来挖掘数据中蕴涵的信息，从表面杂乱无章的海量数据中探究隐藏的规律，为科学发现、工程开发、医学诊疗和商业决策等提供依据。如图 3.3 所示，可视化可以作用于数据科学过程中不同的部分，作为一种人机交互手段，贯穿于整个数据过程。后面的章节将详细阐述这些部分及其和可视化的关系。  
#### 3.2 数据获取和预处理

###### 3.2.1 数据获取
大数据时代的特点之一是数据开始变得廉价，即收集数据的途径多种多样，成本相对低廉。通常来说，数据获取的手段有实验测量、计算机仿真与网络数据传输等。传统的数据获取方式以文件输入/输出为主。在移动互联网时代，基于网络的多源数据交换占据主流。数据获取的挑战主要有数据格式变换和异构异质数据的获取协议两部分。数据的多样性导致不同的数据语义表述，这些差异来自于不同的安全要求、不同的用户类型、不同的数据格式、不同的数据来源。  
数据获取协议（Data Access Protocol，DAP）作为一种通用的数据获取标准，在科研领域应用比较广泛。该协议通过定义基于网络的数据获取句法，以完善数据交换机制，维护、发展和提升数据获取效率。理论上，数据获取协议是一个中立的、不受限于任何规则的协议，它提供跨越规则的句法的互操作性，允许规则内的语义互操作性。数据获取协议以文件为基础，提供数据格式、位置和数据组织的透明度，并以纯 Web 化的方式与网格 FTP/FTP、HTTP、SRB（源路由网桥）、开放地理空间联盟（如 WCS，WMS，WFS）、天文学（如 SIAP，SSAP，STAP）等协议兼容。经过数年发展，第二代数据获取协议 DAP2 已提供了一个与领域无关的网络数据获取协议，业已成为 NASA/ESE 标准，最新的 DAP4 提供了更多的数据类型和传输功能，以适用更广泛的环境，直接满足用户要求。OPeNDAP（http:∥www.opendap.org）是一个研发数据获取协议的组织，它提供了一个同名的科学数据联网的简要框架，允许以本地数据格式快速地获取任意格式远程数据的机制。协议中相关的系统要素包括客户端、浏览器界面、数据集成、服务器等。  
除此之外，互联网上存在大量免费的数据资源，这些资源通常由网站进行维护，并开放专门的 API 使用户得以访问。Google 作为全世界最大的互联网公司之一，提供了许多用于免费数据获取的 API，例如，用于获取高级定制搜索结果的 Google Custom Search，以及用于获取地理坐标信息的 Google Geocoding API 等。 Twitter和 Facebook等社交网络也开放了数据获取 API，用于获取社交网络相关信息。Data source handbook 一书介绍了很多类型的开放 API，其数据类型包括网页、用户信息、搜索关键字、地理信息，以及书籍、音乐等商品信息。
###### 3.2.2 数据预处理
数据获取之后，通常需要进行预处理。常见的数据元素操作如下。
+ 合并   
将两个以上的属性或对象合并为一个属性或对象合并为一个属性或对象。合并操作的效用包括：有效简化数据；改变数据尺度（例如，从乡村起逐级合并，形成城镇、地区、州、国家等）；减少数据的方差。
+ 采样  
采样是统计学的基本方法，也是对数据进行选择的主要手段，在对数据的初步探索和最后的数据分析环节经常被采用。统计学家实施采样操作的根本原因是获取或处理全部数据集的代价太高，或者时间开销无法接受。如果采样结果大致具备原始数据的特征，那么这个采样是具有代表性的。最简单的随机采样可以按某种分布随机从数据集中等概率地选择数据项。当某个数据项被选中后，同一个数据项可能被多次选中。采样也可分层次进行：先将数据全集分为多份，然后在每份中随机采样。
+ 降维  
维度越高，数据集在高维空间的分布越稀疏，从而减弱了数据集的密度和距离的定义对于数据聚类和离群值检测等操作的影响。将数据属性的维度降低，有助于解决维度灾难，减少数据处理的时间和内存消耗；可以更为有效地可视化数据；降低噪声或消除无关特征等。降维是数据挖掘的核心研究内容，常规的做法有主元分析、奇异值分解、局部结构保持的 LLP、ISOMAP 等方法。
+ 特征子集选择
从数据集中选择部分数据属性值可以消除冗余的特征、与任务无关的特征。特征子集选择可达到降维的效果，但不破坏原始的数据属性结构。特征子集选择的方法包括：暴力枚举法、特征重要性选择、压缩感知理论的稀疏表达方法等。
+ 特征生成  
特征生成可以在原始数据集基础上构建新的能反映数据集重要信息的属性。三种常用的方法是：特征抽取、将数据应用到新空间、基于特征融合与特征变换的特征构造。
+ 离散化与二值化  
将数据集根据其分布划分为若干个子类，形成对数据集的离散表达，称为离散化。将数据值映射为二值区间，是数据处理中的常见做法。将数据区间映射到 [0,1] 区间的方法称为归一化。
+ 属性变换  
将某个属性的所有可能值一一映射到另一个空间的做法称为属性变换，如指数变换、取绝对值等。标准化与归一化是两类特殊的属性变换，其中标准化将数据区间变换到某个统一的区间范围，归一化则变换到 [0,1] 区间。

#### 3.3 数据组织与管理
大数据时代随着信息量的飞速增长，我们也不得不面对“无关、错误的信息随之增长”这样一个事实。从数据获取、存储直到最后的分析、可视化，其中很多因素都会造成无关、错误数据的产生和引入，例如，使用不同的数据源可能会直接引入重复或无关的数据；数据处理阶段没有很好地将无关、错误的数据过滤出来；由于资源限制而无法进行海量数据的过滤清理等。因此，我们需要对这些过程进行自动控制，使得数据能够进行有效的组织，进而能够存储起来供后续分析使用。  
数据管理指对数据进行有效的收集、存储、处理和应用的过程。随着计算机技术的发展，数据管理经历了人工管理、文件系统、数据库系统三个阶段。在面向复杂数据的数据可视化过程中，还涉及面向应用的数据管理，它的管理对象是数据生命周期所涉及的应用过程中描述构成应用系统构件属性的元数据，包括流程、文件、数据元、代码、规则、脚本、档案、模型、指标、物理表、ETL、运行状态等。  
数据组织指按一定的方式和规则对数据进行归并、存储、处理的过程。实现数据有效管理的关键是数据组织。从逻辑上看，数据组织具有一个层层相连的层次体系：位、字符、数据元、记录、文件、数据库。其中，记录是逻辑上相关的数据元组合；文件是逻辑上相关的记录集合；数据库是一种作为计算机系统资源共享的数据集合。与数据可视化有关的常用数据组织和管理形式如下。
+ 文件存储  
数据组织和管理的最简单形式是文件。在DBMS（数据库管理系统）出现以前，人们通常以文件作为数据输入和输出的形式。然而，以文件作为数据存储形式有相当多的弊端，例如，数据可能出现冗余、不一致，数据访问烦琐，难以添加数据约束，安全性不高等问题。然而作为一种高度灵活的数据存储形式，它允许使用者非常自由地进行数据处理而不受过多的约束。
+ 结构化文件格式  
为方便！！！！！！！！！
+ 数据库  
数据组织的高级形式是数据库，即存储在计算设备内、有组织的、共享的、统一管理的数据集合。数据库中保存的数据结构既描述了数据间的内在联系，便于数据增加、更新与删除，也保证了数据的独立性、可靠性、安全性与完整性，提高了数据共享程度和数据管理效率。关系数据库模型是当前数据库系统最为常用的数据模型。

下文将按数据采集、存储和处理的顺序描述与数据可视化最相关的数据组织及管理方法。
###### 3.3.1 数据清洗与精简
数据质量是数据采集后所需考虑的第一个问题。对于海量数据来说，未经处理的原始数据中包含大量的无效数据，这些数据在到达存储过程之前就应该被过滤掉。例如,在分析结构化数据时常常需要过滤含有无效值的记录，或者用某种规则对无效值部分进行校正等。  
在原始数据中,常见的数据质量问题包括：噪声和离群值、数值缺失、数值重复等。解决这些问题的方法称为数据清洗( Data Cleaning)。
+ 噪声指对真实数据的修改；离群值指与大多数数据偏离较大的数据。
+ 数值缺失的主要原因包括：信息未被记录；某些属性不适用于所有实例。处理数据缺失的方法有：删除数据对象；插值计算缺失值；在分析时忽略缺失值；用概率模型估算缺失值等。非结构化数据通常存在低质量数据项（如从网页和传感器网络获取的数据），构成了数据清洗和数据可视化的新挑战。
+ 数值重复的主要来源是异构数据源的合并，采用数据清洗方法消除。  

处理数据丢失和重复记录仅是数据清洗的一部分。其他操作还包括：运用汇总统计删除、分辨或者修订错误或不精确的数据；调整数据格式和测量单位；数据标准化与归一化等。另一方面，实际采集的数据经常包含错误和自相矛盾的内容，而且实验、模拟和信息分析过程不可避免地存在误差，从而对分析结果产生很大的影响。通常这类问题可以归结为不确定性。不确定性有两方面内涵，包括各数据点自身存在的不确定性，以及数据点属性值的不确定性。前者可用概率描述，后者有多种描述方式，如描述属性值的概率密度函数，以方差为代表的统计值等。由于不确定性数据与确定性数据存在显著差异,所以针对不确定性数据需要采取特殊的数据建模、分析和可视化方法。  
可视化作为一种有效的展示手段和交互手段,在数据清洗中发挥了巨大的作用。有人提出 33 种脏数据类型，并且强调其中的 25 种在清理时需要人的交互。这意味着多种脏数据在清理时可使用交互式可视化方法来提高数据清理效率，如图 3.4 所示。  
由高维性带来的维度灾难、数据的稀疏性和特征的多尺度性是大数据时代中数据所特有的性质。直接对海量高维的数据集进行可视化通常会产生杂乱无章的结果，这种现象被称为视觉混乱。为了能够在有限的显示空间内表达比显示空间尺寸大得多的数据，我们需要进行数据精简。在数据存储、分析层面进行的数据精简能降低数据复杂度，减少数据点数目并同时保留数据中的内涵特征，从而减少查询和处理时的资源开销，提高分析的响应性能。经典的数据精简包括前面章节中描述的方法，如统计分析、采样、直方图、聚类和













###### 3.3.2 数据整合与集成

###### 3.3.3 数据库

#### 3.4 数据分析与挖掘

###### 3.4.1 探索式数据分析

###### 3.4.2 联机分析处理

###### 3.4.3 数据挖掘


#### 3.5 数据工作流

#### 3.6 数据科学的挑战
数据科学是大数据时代应运而生的一门新学科。围绕数据处理的各个学科方向都开始遇到前所未有的挑战。
+ 作为数据获取和存储基础的计算机科学，由于数据成本急剧下降导致的数据量急剧增长、数据复杂程度飞速上升，如何有效地获取数据、有效地处理数据获取的不确定性、对原始数据进行清理、分析，进而高效地完成数据存储和访问，达到去重，去粗取精的目的，是急需解决的问题。同时，如何构建结构化数据与非结构化数据之间的有效关联及语义信息，使得异构、多源数据之间的关系能够得以存储并支持后续分析,并提供足够的性能保证,也是面临的挑战之一。
+ 对于统计、分析、数据挖掘研究者，将大数据变“小”，即从大数据中获取更加有效的信息和知识，是研究重点。这需要考虑理论与工程算法两个方面的问题。
+ 可视化作为数据科学中不可或缺的重要一环，也开始高度关注大数据带来的问题，其中包括：高维数据可视化，复杂、异构数据可视化，针对海量数据的实时交互设计，分布式协同可视化，以及针对大数据的可视分析流程等。